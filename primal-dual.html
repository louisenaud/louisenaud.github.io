<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Louise Naud">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- For code display: Code Prettify -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>


    <title>Primal Dual</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <!-- Configuration for Mathjax and AlgoType-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            extensions: ["tex2jax.js", "color.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            "HTML-CSS": { availableFonts: ["TeX"] },
            TeX: {
                Macros: {
                            And:     "\\\\mathbf{and}",
                            Or:      "\\\\mathbf{or}",
                            Not:     "\\\\mathbf{not}",
                            Is:      "\\\\mathbf{is}",
                            In:      "\\\\mathbf{in}",
                            Mapped:  "\\\\mathbf{mapped}",
                            Nil:     "\\\\mathbf{nil}"
                }
            }
        });
    </script>
    <!-- KateX for math formulas in pseudo code -->
    <link rel="stylesheet" href="katex/katex.min.css">
    <script src="katex/katex.min.js"></script>
    <!-- Stylesheet for pseudo code -->
    <link rel="stylesheet" href="pseudocode/pseudocode.min.css">
    <script src="pseudocode/pseudocode.min.js"></script>
    <!-- Library for Pseudo Code -->
    <link rel="stylesheet" href="Algotype.js/algotype_my_config.css">
    <script type="text/javascript" async
            src="Algotype.js/algotype.js">
    </script>
</head>




<body>

<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                Menu <i class="fa fa-bars"></i>
            </button>
            <a class="navbar-brand" href="index.html">ML Food for the Soul</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="index.html">Home</a>
                </li>
                <li>
                    <a href="about.html">About</a>
                </li>
                <li>
                    <a href="blog-tickets-list.html">Posts</a>
                </li>
                <li>
                    <a href="contact.html">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

<!-- Page Header -->
<!-- Set your background image for this header on the line below. -->
<header class="intro-header" style="background-image: url('img/saddle.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>First Order Primal Dual Techniques</h1>
                    <h2 class="subheading">A versatile framework for a wide variety of convex optimization problems
                        .</h2>
                    <span class="meta">Posted by <a href="#">Louise</a> on July 4, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>In this post, we are going to discuss a very powerful -mathematically as well as pragmatically-
                    optimization framework; the <i>First Order Primal Dual Convex Optimization Technique</i>.
                    This involves a few definitions that may not look friendly at first, but the framework is in
                    fact relatively understandable. The framework has been -successfully- applied to image denoizing
                and image inpainting, and is really worthy of spending some time to comprehend it. </p>

                <p>We will remind a couple of definitions that are useful in this framework, but you can find many
                    more details, definitions, theorems and proofs in <a hreh="http://www.springer.com/us/book/9783540422051">"Convex Analysis"</a> by
                    Hiriart-Urruty and Lemarchal, <a
                            href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">"Convex Optimization"</a> by Boyd and Vandenberghe or <a
                            href="http://iie.fing.edu.uy/~pmuse/docs/course_Mila/Optim-MVD12.pdf">"Optimization. Applications in
                        image processing."</a> by Nikolova (the latter two are available freely in pdf format).</p>

                <p>We are going to consider here a general problem of the form:
                $$
                    \begin{align}
                    & \underset{x}{\text{minimize}} & & \sum_{i \in \Omega} M_i(x_i) + R(Lx)
                    \end{align}
                    $$
                    with:
                    <ul>
                <li>\(M_i\) a lower semi continuous function </li>
                <li>\(R\) a lower semi continuous function</li>
                <li>\(L\) a continuous linear operator</li>
            </ul>
                </p>

                <h2 class="section-heading">Soooo.... Why is this useful, again?</h2>
                <p>Let's say you need to denoise an image for instance. In the Total Variation context, you can write
                    image denoising as the following
                optimization
                problem:
                $$
                \begin{align*}
                & \underset{x}{\text{minimize}}
                & & \left\| \nabla x\right\| + \frac{\lambda}{2} \left\| x -f \right\|^2
                \end{align*}
                $$
                </p>
                <p>The Total variation framework also allows for a nice formulation of the motion estimation problem
                    . </p>

                <p>It can also be used for inpainting:</p>

                <h2 class="section-heading">Useful definitions</h2>
                <p>Let's first define a couple of concept that we will use afterwards. A function \(f\) on a normed
                    real vector space \(V\) is <i>proper</i> if \(f: V \rightarrow ]-\infty ,
                    \infty] \) and if it is not identically equal to \(+\infty \).</p>
                <p>A function \(f\) on a normed real vector space \(V\) is <i>coercive</i> if \(\lim_{\left\|u \right\|
                    \rightarrow + \infty} f(u) = + \infty\). </p>
                <p>Most of the functions we will consider here will be <i>lower semi-continuous</i>. This means that
                    a function \(f\) on a real topological space \(\mathcal{X}\), \(f: V \rightarrow ]-\infty ,
                    \infty] \) is lower semi-continuous (l.s.c.) if \( \forall \lambda \in \mathbb{R}\), the set \(
                    {u \in V: f(u) \leq \lambda}\) is closed in V. </p>
                <p>This property is quite important, as we will see in the following theorem: Let \(U\) be non-empty
                    and closed, and \( f : \mathbb{R}^n \rightarrow \mathbb{R}\) be l.s.c.and proper. If \(U\) is
                    not bounded, we also suppose that f is coercive. Then: \(\exists \hat{u} \in U s.t. f(\hat{u}) =
                    inf_{u \in U} f(u)\). </p>
                <p>This is actually very important; it means that if \(f\) has all the aforementioned properties,
                    it takes a minimum value on any set \(U\) non-empty and closed.</p>

                <a href="#">
                    <img class="img-responsive" src="img/lsc.png" alt="">
                </a>
                <p>We will suppose here that you are familiar with the concept of convex functions and sets.</p>

                <h2 class="section-heading">The Moreau Envelope</h2>
                <p>In our image denoising or inpainting context, the function to optimize may not be smooth, and this
                complicates the task of minimizing/optimizing the objective function. In order to alleviate this
                difficulty, a brilliant idea has been introduced in "Proximité et dualité dans un espace hilbertien"
                    by Moreau.
                Given a proper lsc convex function \( F : \mathcal{X} \leftarrow \mathbb{R} \), the Moreau envelope constructs a smoothed version of function \(F\) where the degree of smoothing is controlled by a parameter \(\tau\). The Moreau envelope is defined as the unique solution of the optimization problem:
                $$
                \begin{align}
                \mathcal{M}_{F, \tau} (\tilde{x}) = \min \limits_{x \in \mathcal{X}} F(x) + \frac{1}{2 \tau} \left\| x - \tilde{x} \right\|_2^2
                \end{align}
                    $$</p>
                <p>We note that this problem is smooth and it admits a unique minimizer due to the added smoothing
                    \(\mathcal{l}_2\) norm term.</p>
                <p>This being said, just because a problem is strictly convex doesn't mean it is easy to solve. So
                basically, if \( F \) is hard to optimize, the Moreau envelope is likely difficult to compute.
                    Nonetheless, there exist some functions whose Moreau envelope computation yields a closed form.</p>

                <p>We can associate to this function the following operator:
                    $$
                    \begin{align}
                    prox_{F, \tau} (\tilde{x}) = \arg \min \limits_{x \in \mathcal{X}} F(x) + \frac{1}{2 \tau} \left\|
                    x - \tilde{x} \right\|_2^2
                    \end{align}
                    $$
                    that returns the unique minimizer of the Moreau envelope. This operator is called the <i>proximal
                        operator.
                    </i>
                </p>

                <p>Not additive.</p>

                <h2 class="section-heading">The Fenchel Transform</h2>
                <p>Another difficulty of this optimization problem is the coupling of variables \(x_i\) due to the
                    linear operator \(L\) and the function \(R\). Combined to the eventual non smoothness of \(R\), this
                    imposes to optimize all variables \(x_i\) jointly.
                    One solution would be to smooth the objective function \(R\) to make corners disappear. However, this
                    changes the objective function which might not be desirable. Using the Moreau envelope is
                    unfortunately generally computationally very intensive because, as seen in Section 2.3.1, the
                    proximal operator is not an additive function. The first order primal dual technique relies
                    on the Fenchel transformation to decouple the variables. This transformation comes with the cost
                    of adding new variables, named dual variables, to our current optimization problem. The Fenchel
                    transform augments to a <i>saddle point optimization problem</i>, that is to say, a convex
                    minimization problem for the primal and a concave maximization problem for the dual variables.</p>

                <p>The <i>Fenchel Transform</i>, also called <i>convex conjugate</i> of a function \(R\) is defined
                    as follows:
                    $$
                    \begin{align}
                    R^{*}(y) = \sup \limits_{x} \langle x, y \rangle - R(x)
                    \end{align}
                    $$
                    This transformation encodes the convex hull of the function \(R\) epigraph as a set of
                    hyperplanes. It is worth noting that the Fenchel transform always yields a convex function.
                </p>
                <!-- TODO(louise): illustrations!! -->
                <p>Applying this transform twice to the function \(R\) yields the bi-convex conjugate \( R^{**} \):
                    $$
                    \begin{align}
                    R^{**}(y) = \sup \limits_{y} \langle x, y \rangle - R*(y)
                    \end{align}
                    $$
                    So if \(R\) is a convex function, \( R^{**} = R \) and we have:
                    $$
                    \begin{align}
                    R(x) = \sup \limits_{y} \langle x, y \rangle - R*(y)
                    \end{align}
                    $$
                    And this gives an efficient approach to decouple the variable \(x\) in our context!!
                </p>

                <h2 class="section-heading">Primal Dual algorithm</h2>
                <p>So now let's see how to combine the last two ideas in our optimization problem.
                First, we introduce the Moreau envelope for the \( (M_i)_{i} \) :
                $$ \begin{align}
                    \arg \inf \limits{x \in \mathcal{X}} \sum_{i} M_i (x_i) + \frac{1}{\tau} \left\| x_i -
                    \tilde{x}_i \right\|^2_2 + R(Lx)
                    \end{align}
                $$
                Then we apply the Fenchel Transform to \(R\):
                $$
                \begin{align}
                \arg \inf \limits{x \in \mathcal{X}} \sum_{i} M_i (x_i) + \frac{1}{2\tau} \left\| x_i -
                \tilde{x}_i \right\|^2_2 + \langle y , Lx \rangle - R^{*}(y)
                \end{align}
                $$
                    So now, all the \(x_i\) are decoupled. Their minimization is achieved by computing a proximal
                    operator. But \(R^{*}\) is not assured to be smooth, so we are going to use the Moreau envelope
                    again on \(R^{*}\):
                    $$
                    \begin{align}
                    \arg \inf \limits{x \in \mathcal{X}} \sum_{i} M_i (x_i) + \frac{1}{2\tau} \left\| x_i -
                    \tilde{x}_i \right\|^2_2 + \langle y , Lx \rangle - R^{*}(y) - \frac{1}{2
                    \sigma} \left\| y - \tilde{y} \right\|^2_2
                    \end{align}
                    $$
                    And now, we can transform this problem into an iterative one by doing:
                    $$
                    \begin{align}
                    \arg \inf \limits{x \in \mathcal{X}} \sum_{i} M_i (x_i^{(n)}) + \frac{1}{2\tau} \left\| x_i^{(n)} -
                    x_i^{(n-1)} \right\|^2_2 + \langle y^{(n)} , Lx^{(n)} \rangle - R^{*}(y^{(n)}) - \frac{1}{2
                    \sigma} \left\| y^{(n)} - y^{(n-1)} \right\|^2_2
                    \end{align}
                    $$
                    So now, we have, for all \(n\), a problem that is convex with respect to \(x^{(n)}\) and concave
                    with respect to \( y^{(n)}\); we have a saddle point problem formulation! And we know that the
                    two series converge to a fixed saddle point.

                </p>

                <p>Now that we finally have a problem that is iteratively optimizable, let's see the pseudo code of
                    the Primal Dual Algorithm:
                <pre class="pseudocode">
\begin{algorithm}
\caption{First Order Primal Dual Algorithm}
\begin{algorithmic}
\PROCEDURE{primal-dual}{$M_i, R, L, data$}
    \STATE Compute $R^{*}$
    \STATE Initialize Primal variable $x^0 \leftarrow 0$
    \STATE Initialize Dual variable $y^0 \leftarrow 0$
    \STATE $\tilde{x} \leftarrow 0$
    \WHILE{Stopping Criteria is not met}
        \STATE {Optimize dual variable $y^{(n+1)} \leftarrow prox_{\sigma (\langle y, L\tilde{x} \rangle - R^{*}
                (y^{(n)}))} (y^{(n)})$}
        \STATE {Optimize the primal variable $x^{(n+1)} \leftarrow prox_{\tau(\sum_i M_i(x_i) + \langle
                L^{T}y^{(n)}, x \rangle) (x^{(n)})}$}
        \STATE {Smooth Variable $\tilde{x} \leftarrow x^{(n+1)} + \theta (x^{(n+1)} - x^{(n)})$}
    \ENDWHILE

    \RETURN $x^{(n)}$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</pre>
                </p>

                <h2 class="section-heading">A real life example</h2>



                <p>You can leave comments down here, or contact me through the contact form of this blog if you
                    have questions or remarks on this post!</p>
                <!-- Blog Comments -->

                <!-- begin wwww.htmlcommentbox.com -->
                <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">Widget</a> is loading comments...</div>
                <link rel="stylesheet" type="text/css" href="//www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
                <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="//www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%24rbyy1KYA6pUs1Jh9BfMHZ."+"&opts=16862&num=10&ts=1498161055918");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
                <!-- end www.htmlcommentbox.com -->


            </div>
        </div>
    </div>
    </div>
    </div>
</article>

<hr>

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <li>
                        <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                </ul>
                <p class="copyright text-muted">Copyright &copy; Your Website 2016</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="vendor/jquery/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="vendor/bootstrap/js/bootstrap.min.js"></script>

<!-- Contact Form JavaScript -->
<script src="js/jqBootstrapValidation.js"></script>
<script src="js/contact_me.js"></script>

<!-- Theme JavaScript -->
<script src="js/clean-blog.min.js"></script>

<script>
    var blocks = document.getElementsByClassName("pseudocode");
    for(var blockId = 0; blockId < blocks.length; blockId++) {
        var block = blocks[blockId];

        var code = block.textContent;
        var options = {
            lineNumber: true
        };

        var outputEl = document.createElement('div');
        outputEl.className += " pseudocode-out";
        block.parentNode.insertBefore(outputEl, block.nextSibling);

        pseudocode.render(code, outputEl, options);
    }

    while( blocks[0]) {
        blocks[0].parentNode.removeChild(blocks[0]);
    }
</script>
<!-- Google Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52868500-2', 'auto');
    ga('send', 'pageview');

</script>

</body>

</html>
