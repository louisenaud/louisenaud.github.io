<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Louise Naud">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- For code display: Code Prettify -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>


    <title>Time Series Prediction</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
    <!-- Configuration for Mathjax and AlgoType-->
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js", "color.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] },
    TeX: {
        Macros: {
                    And:     "\\\\mathbf{and}",
                    Or:      "\\\\mathbf{or}",
                    Not:     "\\\\mathbf{not}",
                    Is:      "\\\\mathbf{is}",
                    In:      "\\\\mathbf{in}",
                    Mapped:  "\\\\mathbf{mapped}",
                    Nil:     "\\\\mathbf{nil}"
        }
    }
});
</script>
    <!-- KateX for math formulas in pseudo code -->
    <link rel="stylesheet" href="katex/katex.min.css">
    <script src="katex/katex.min.js"></script>
    <!-- Stylesheet for pseudo code -->
    <link rel="stylesheet" href="pseudocode/pseudocode.min.css">
    <script src="pseudocode/pseudocode.min.js"></script>
    <!-- Library for Pseudo Code -->
    <link rel="stylesheet" href="Algotype.js/algotype_my_config.css">
    <script type="text/javascript" async
            src="Algotype.js/algotype.js">
    </script>
    <!--Bokeh-->
    <link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css" type="text/css" />

    <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js"></script>
    <script type="text/javascript">
        Bokeh.set_log_level("info");
    </script>
    <!--D3js for visualization-->
    <script src="https://d3js.org/d3.v4.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
</head>


<body>

<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                Menu <i class="fa fa-bars"></i>
            </button>
            <a class="navbar-brand" href="index.html">ML Food for the Soul</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="index.html">Home</a>
                </li>
                <li>
                    <a href="about.html">About</a>
                </li>
                <li>
                    <a href="blog-tickets-list.html">Posts</a>
                </li>
                <li>
                    <a href="contact.html">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

<!-- Page Header -->
<!-- Set your background image for this header on the line below. -->
<header class="intro-header" style="background-image: url('img/primal-dual-net/figure_CG.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Time Series Prediction.</h1>
                    <h2 class="subheading">How to predict the future -value-.</h2>
                    <span class="meta">Posted by <a href="contact.html">Louise</a> on February 8, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>In this post we are going to go through classic methods for predicting time series. </p>

                <h2 class="section-heading">The data</h2>
                <p>A very classic data type for time series are stock prices. We are going to focus on 'AAPL', 'AMZN', 'GOOGL' and 'FB'</p>
                <p>We are going to consider for each data point the previous \(T\) points as a fixed size sub time sequence, as represented in the graph below when hovering the mouse on it.</p>

                <div class="bk-root">
                    <div class="bk-plotdiv" id="035e5c7e-7aba-48a2-a387-b0170e954bab"></div>
                </div>

                <!--Bokeh plot -->
                <script type="application/json" id="d71ebe9c-b9d7-4a09-a4e1-91cbd7717655">
                    {"edb6f766-2d2f-49d6-8afa-dc46cffb690d":{"roots":{"references":[{"attributes":{"callback":null,"column_names":["y1","y0","x0","x1"],"data":{"x0":[],"x1":[],"y0":[],"y1":[]}},"id":"dff16aee-f79c-4ab8-afff-144b9d0ebabf","type":"ColumnDataSource"},{"attributes":{"mantissas":[1,2,5],"max_interval":500.0,"num_minor_ticks":0},"id":"ca21c09c-57f9-43f4-979b-397ad8a1d7c7","type":"AdaptiveTicker"},{"attributes":{"days":[1,4,7,10,13,16,19,22,25,28]},"id":"4d3b9a7e-a6bb-4e25-ad8b-7fba9093c1ff","type":"DaysTicker"},{"attributes":{"months":[0,4,8]},"id":"2b87cd25-0a8a-4130-83c2-b5651ac2a2b9","type":"MonthsTicker"},{"attributes":{"axis_label":"Date","formatter":{"id":"45e3d23a-dc4e-410b-99de-9df208f92998","type":"DatetimeTickFormatter"},"plot":{"id":"ca854178-19e2-440a-a4f5-e532f51c8d20","subtype":"Figure","type":"Plot"},"ticker":{"id":"1b5096d7-2dd3-4ab5-9ec7-d0efbc28f463","type":"DatetimeTicker"}},"id":"e7ee9e6e-6b9d-428b-8329-5f459761f58a","type":"DatetimeAxis"},{"attributes":{"axis_label":"AAPL Price","formatter":{"id":"1eeeb553-ab7d-4a27-b4f8-e3b207b00f20","type":"BasicTickFormatter"},"plot":{"id":"ca854178-19e2-440a-a4f5-e532f51c8d20","subtype":"Figure","type":"Plot"},"ticker":{"id":"945a8d15-e462-452f-bf03-8523bcc2da68","type":"BasicTicker"}},"id":"303d804a-29a2-4340-8d9f-e26010685351","type":"LinearAxis"},{"attributes":{"fill_alpha":{"value":0.3},"fill_color":{"value":"deepskyblue"},"line_alpha":{"value":0.3},"line_color":{"value":"deepskyblue"},"size":{"units":"screen","value":8},"x":{"field":"x"},"y":{"field":"y"}},"id":"c81d8ac8-7b11-4fe5-8889-913cb8d35cd9","type":"Circle"},{"attributes":{"grid_line_alpha":{"value":0.3},"plot":{"id":"ca854178-19e2-440a-a4f5-e532f51c8d20","subtype":"Figure","type":"Plot"},"ticker":{"id":"1b5096d7-2dd3-4ab5-9ec7-d0efbc28f463","type":"DatetimeTicker"}},"id":"e1ccd28a-5214-4dee-8516-4feed9d7409b","type":"Grid"},{"attributes":{"source":{"id":"dff16aee-f79c-4ab8-afff-144b9d0ebabf","type":"ColumnDataSource"}},"id":"4e9f9012-648f-4ee9-85bf-b973947ece95","type":"CDSView"},{"attributes":{"data_source":{"id":"dff16aee-f79c-4ab8-afff-144b9d0ebabf","type":"ColumnDataSource"},"glyph":{"id":"9551208a-30f7-417a-9711-4ce95e886f8a","type":"Segment"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"a87eabdb-01ba-4fe2-8f12-6545c0f5bf0b","type":"Segment"},"selection_glyph":null,"view":{"id":"4e9f9012-648f-4ee9-85bf-b973947ece95","type":"CDSView"}},"id":"286b7d90-e904-4627-b6be-3507705d73eb","type":"GlyphRenderer"},{"attributes":{"days":[1,8,15,22]},"id":"2b721274-38ed-4bf5-b828-bb41a0d415b2","type":"DaysTicker"},{"attributes":{"months":[0,6]},"id":"596fce41-3f17-4348-b4a8-0b94347a57ed","type":"MonthsTicker"},{"attributes":{"callback":{"id":"f982c475-f544-4d44-8da4-5e8b35c33e55","type":"CustomJS"},"mode":"mouse","renderers":[{"id":"0af4cb03-80ab-48c0-a45b-d47cd3f4fdbc","type":"GlyphRenderer"}],"tooltips":null},"id":"c3f9f835-6891-4e7c-807f-06e37ca32a2f","type":"HoverTool"},{"attributes":{"line_alpha":{"value":0.9},"line_color":{"value":"firebrick"},"line_width":{"value":3},"x0":{"field":"x0"},"x1":{"field":"x1"},"y0":{"field":"y0"},"y1":{"field":"y1"}},"id":"9551208a-30f7-417a-9711-4ce95e886f8a","type":"Segment"},{"attributes":{"args":{"circle":{"id":"a55dd8d8-a9ed-4097-9e27-6963176cba55","type":"ColumnDataSource"},"segment":{"id":"dff16aee-f79c-4ab8-afff-144b9d0ebabf","type":"ColumnDataSource"}},"code":"\nvar data = {'x0': [], 'y0': [], 'x1': [], 'y1': []};\nvar cdata = circle.data;\nvar indices = cb_data.index['1d'].indices;\nfor (i=0; i &lt; indices.length; i++) {\nind0 = indices[i]\nfor (j=ind0-20; j &lt; ind0; j++) {\ndata['x0'].push(cdata.x[j]);\ndata['y0'].push(cdata.y[j]);\ndata['x1'].push(cdata.x[j+1]);\ndata['y1'].push(cdata.y[j+1]);\n}\n}\nsegment.data = data;\n"},"id":"f982c475-f544-4d44-8da4-5e8b35c33e55","type":"CustomJS"},{"attributes":{},"id":"3b5a6268-5d63-4057-a524-f6bf4fff4c6f","type":"LinearScale"},{"attributes":{"base":24,"mantissas":[1,2,4,6,8,12],"max_interval":43200000.0,"min_interval":3600000.0,"num_minor_ticks":0},"id":"40e0f61c-3cfe-410e-a5a3-55cb1a80c0d9","type":"AdaptiveTicker"},{"attributes":{"base":60,"mantissas":[1,2,5,10,15,20,30],"max_interval":1800000.0,"min_interval":1000.0,"num_minor_ticks":0},"id":"4cd4d5b9-33e7-42c4-b164-322c6c6a727a","type":"AdaptiveTicker"},{"attributes":{"dimension":1,"grid_line_alpha":{"value":0.3},"plot":{"id":"ca854178-19e2-440a-a4f5-e532f51c8d20","subtype":"Figure","type":"Plot"},"ticker":{"id":"945a8d15-e462-452f-bf03-8523bcc2da68","type":"BasicTicker"}},"id":"ee86aa84-10c9-4304-baee-9e7e0eabd256","type":"Grid"},{"attributes":{},"id":"945a8d15-e462-452f-bf03-8523bcc2da68","type":"BasicTicker"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"value":"#1f77b4"},"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"size":{"units":"screen","value":10},"x":{"field":"x"},"y":{"field":"y"}},"id":"51b243f7-65f8-4b52-aa99-1f9d23a61af8","type":"Circle"},{"attributes":{"months":[0,1,2,3,4,5,6,7,8,9,10,11]},"id":"3bf4c061-5d2f-4067-96aa-19138b8dc360","type":"MonthsTicker"},{"attributes":{"data_source":{"id":"a55dd8d8-a9ed-4097-9e27-6963176cba55","type":"ColumnDataSource"},"glyph":{"id":"c81d8ac8-7b11-4fe5-8889-913cb8d35cd9","type":"Circle"},"hover_glyph":{"id":"79708256-894b-4182-b342-c4142f18c578","type":"Circle"},"muted_glyph":null,"nonselection_glyph":{"id":"51b243f7-65f8-4b52-aa99-1f9d23a61af8","type":"Circle"},"selection_glyph":null,"view":{"id":"2b143534-496c-493a-96d8-05a3a0f7d212","type":"CDSView"}},"id":"0af4cb03-80ab-48c0-a45b-d47cd3f4fdbc","type":"GlyphRenderer"},{"attributes":{"callback":null},"id":"abb213ff-a040-4d2b-89f3-6b6376126d8e","type":"DataRange1d"},{"attributes":{"callback":null},"id":"dfbe01d0-dd8e-4408-891a-59a79895608a","type":"DataRange1d"},{"attributes":{},"id":"35723859-5954-4869-a284-69a22909a2bb","type":"ResetTool"},{"attributes":{"months":[0,2,4,6,8,10]},"id":"577baa72-2584-421e-96a7-145834b7e44d","type":"MonthsTicker"},{"attributes":{"source":{"id":"a55dd8d8-a9ed-4097-9e27-6963176cba55","type":"ColumnDataSource"}},"id":"2b143534-496c-493a-96d8-05a3a0f7d212","type":"CDSView"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"441fa756-fc3b-4cd0-a2cf-a33e9af6a7e7","type":"BoxAnnotation"},{"attributes":{},"id":"a121e806-60c4-4e77-bf0c-84787a2ee993","type":"YearsTicker"},{"attributes":{"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"line_width":{"value":3},"x0":{"field":"x0"},"x1":{"field":"x1"},"y0":{"field":"y0"},"y1":{"field":"y1"}},"id":"a87eabdb-01ba-4fe2-8f12-6545c0f5bf0b","type":"Segment"},{"attributes":{},"id":"538c4637-5b4f-405d-a720-909920d2c818","type":"LinearScale"},{"attributes":{"fill_alpha":{"value":0.9},"fill_color":{"value":"firebrick"},"line_alpha":{"value":0.9},"line_color":{"value":"firebrick"},"size":{"units":"screen","value":10},"x":{"field":"x"},"y":{"field":"y"}},"id":"79708256-894b-4182-b342-c4142f18c578","type":"Circle"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"d329afdd-fe88-467e-b42f-ff3181bd00f8","type":"BoxZoomTool"},{"id":"35723859-5954-4869-a284-69a22909a2bb","type":"ResetTool"},{"id":"c3f9f835-6891-4e7c-807f-06e37ca32a2f","type":"HoverTool"}]},"id":"e901440b-4d7b-439a-9c3b-4e2ce83d5702","type":"Toolbar"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"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","dtype":"float64","shape":[500]},"y":[127.46,124.63,122.18,123.82,121.54,119.46,119.74,119.15,122.96,129.91,125.02,121.73,124.62,124.49,120.93,122.25,119.69,127.35,126.03,127.94,126.61,126.73,132.82,129.67,133.27,139.53,140.98,145.06,140.25,143.01,143.5,149.53,147.49,151.61,153.08,155.89,152.84,151.44,154.55,147.14,147.78,148.38,153.7,154.49,161.04,168.16,160.2,162.89,168.94,169.73,172.24,175.05,173.95,180.0,180.94,184.73,186.66,182.59,185.06,183.45,188.16,189.96,186.26,189.73,187.62,183.6,185.9,178.19,177.05,181.17,186.43,187.01,186.69,188.75,186.1,185.37,185.19,189.43,185.64,181.61,185.64,180.81,173.26,172.37,176.84,181.43,178.75,180.9,175.27,173.16,173.25,177.39,168.26,170.09,167.44,174.68,168.18,170.12,175.16,179.55,174.25,176.63,172.58,173.88,169.64,172.81,171.81,165.15,166.29,162.02,166.26,159.03,162.12,154.4,157.08,159.88,158.95,156.66,153.23,160.64,164.19,163.57,169.55,173.56,176.73,179.3,179.32,175.74,175.39,173.53,175.84,174.29,176.79,172.55,173.64,174.67,173.74,169.53,166.19,166.96,161.22,160.18,157.92,151.68,151.61,152.65,148.94,140.36,139.88,127.83,134.09,140.91,131.05,126.84,128.71,131.93,128.24,105.26,113.66,109.12,100.1,97.07,98.14,89.16,89.79,88.74,96.8,110.26,104.08,97.95,101.89,97.4,98.44,91.49,96.87,98.23,96.38,92.09,99.91,104.55,111.04,107.59,106.96,110.99,103.3,99.1,98.24,95.88,94.77,90.12,96.44,90.24,88.14,89.91,86.29,80.49,82.58,92.95,90.8,95.0,92.67,88.93,92.47,95.9,91.41,94.0,99.72,100.06,98.21,95.0,98.27,94.75,95.43,89.16,89.43,90.0,85.74,86.38,85.04,85.81,86.61,86.29,85.35,90.75,94.58,93.02,91.01,92.7,90.58,88.66,87.71,85.33,83.38,82.33,78.2,82.83,88.36,88.36,89.64,90.73,94.2,93.0,90.13,91.51,92.98,93.55,96.46,99.72,102.51,97.83,96.82,99.27,99.16,94.53,94.37,90.64,91.2,86.95,90.25,91.16,89.19,89.31,87.94,88.37,91.17,88.84,85.3,83.11,88.63,92.68,96.35,95.93,95.42,99.66,101.52,101.62,101.59,107.66,106.5,106.49,109.87,106.85,104.49,105.12,108.69,112.71,115.99,118.45,115.0,116.32,119.57,120.22,118.31,117.64,121.45,123.42,120.5,121.76,121.51,125.4,123.9,124.73,123.9,125.14,125.83,127.24,132.07,132.71,132.5,129.06,129.19,129.57,124.42,119.49,122.95,122.42,126.65,127.45,125.87,124.18,122.5,130.78,133.05,135.07,135.81,139.35,139.49,140.95,143.74,144.67,143.85,142.72,140.25,139.95,136.97,136.09,136.35,135.58,135.88,139.48,137.37,134.01,136.22,139.86,142.44,141.97,142.43,142.83,140.02,138.61,135.4,137.22,136.36,138.52,142.34,142.27,146.88,147.52,151.75,152.91,151.51,156.74,157.82,159.99,160.1,160.0,160.03,162.79,163.39,166.43,165.55,165.11,163.91,165.51,164.72,162.83,165.31,168.42,166.78,159.59,164.0,164.6,166.33,169.22,169.06,169.4,167.41,169.45,170.05,168.21,165.3,165.18,166.55,170.31,172.93,171.14,172.56,172.16,173.72,175.16,181.87,184.55,185.02,184.02,184.48,185.5,183.82,182.37,186.15,185.38,185.35,180.86,184.9,186.02,190.01,190.25,189.27,190.47,190.81,190.02,191.29,190.56,188.05,189.86,198.76,204.92,205.2,203.94,202.48,197.37,192.4,196.35,188.5,189.31,188.75,190.81,194.03,194.34,201.46,202.98,203.25,201.99,204.45,206.63,207.0,205.96,200.51,199.92,205.88,204.44,204.19,200.59,199.91,196.97,196.23,196.48,193.32,188.95,189.87,197.8,196.43,194.67,196.98,194.17,195.03,191.86,195.43,198.23,200.36,202.1,209.04,211.61,209.1,211.64,210.73,214.01,214.38,210.97,210.58,211.98,210.11,207.72,210.65,209.43,205.93,215.04,211.73,208.07,197.75,203.07,205.94,207.88,199.29,192.06,194.73,195.86,199.23,192.05,195.46,194.12]}},"id":"a55dd8d8-a9ed-4097-9e27-6963176cba55","type":"ColumnDataSource"},{"attributes":{"days":[1,15]},"id":"b43b9ed4-a1e9-462a-881c-5e318fb9aafe","type":"DaysTicker"},{"attributes":{"below":[{"id":"e7ee9e6e-6b9d-428b-8329-5f459761f58a","type":"DatetimeAxis"}],"left":[{"id":"303d804a-29a2-4340-8d9f-e26010685351","type":"LinearAxis"}],"plot_height":500,"plot_width":800,"renderers":[{"id":"e7ee9e6e-6b9d-428b-8329-5f459761f58a","type":"DatetimeAxis"},{"id":"e1ccd28a-5214-4dee-8516-4feed9d7409b","type":"Grid"},{"id":"303d804a-29a2-4340-8d9f-e26010685351","type":"LinearAxis"},{"id":"ee86aa84-10c9-4304-baee-9e7e0eabd256","type":"Grid"},{"id":"441fa756-fc3b-4cd0-a2cf-a33e9af6a7e7","type":"BoxAnnotation"},{"id":"286b7d90-e904-4627-b6be-3507705d73eb","type":"GlyphRenderer"},{"id":"0af4cb03-80ab-48c0-a45b-d47cd3f4fdbc","type":"GlyphRenderer"}],"title":{"id":"947a3e9a-9950-473a-b4c7-0276bf7903af","type":"Title"},"toolbar":{"id":"e901440b-4d7b-439a-9c3b-4e2ce83d5702","type":"Toolbar"},"toolbar_location":"right","x_range":{"id":"dfbe01d0-dd8e-4408-891a-59a79895608a","type":"DataRange1d"},"x_scale":{"id":"538c4637-5b4f-405d-a720-909920d2c818","type":"LinearScale"},"y_range":{"id":"abb213ff-a040-4d2b-89f3-6b6376126d8e","type":"DataRange1d"},"y_scale":{"id":"3b5a6268-5d63-4057-a524-f6bf4fff4c6f","type":"LinearScale"}},"id":"ca854178-19e2-440a-a4f5-e532f51c8d20","subtype":"Figure","type":"Plot"},{"attributes":{"overlay":{"id":"441fa756-fc3b-4cd0-a2cf-a33e9af6a7e7","type":"BoxAnnotation"}},"id":"d329afdd-fe88-467e-b42f-ff3181bd00f8","type":"BoxZoomTool"},{"attributes":{},"id":"1eeeb553-ab7d-4a27-b4f8-e3b207b00f20","type":"BasicTickFormatter"},{"attributes":{"plot":null,"text":"AAPL close price"},"id":"947a3e9a-9950-473a-b4c7-0276bf7903af","type":"Title"},{"attributes":{},"id":"45e3d23a-dc4e-410b-99de-9df208f92998","type":"DatetimeTickFormatter"},{"attributes":{"num_minor_ticks":5,"tickers":[{"id":"ca21c09c-57f9-43f4-979b-397ad8a1d7c7","type":"AdaptiveTicker"},{"id":"4cd4d5b9-33e7-42c4-b164-322c6c6a727a","type":"AdaptiveTicker"},{"id":"40e0f61c-3cfe-410e-a5a3-55cb1a80c0d9","type":"AdaptiveTicker"},{"id":"f6fdca82-13da-4926-a805-2aab51646c80","type":"DaysTicker"},{"id":"4d3b9a7e-a6bb-4e25-ad8b-7fba9093c1ff","type":"DaysTicker"},{"id":"2b721274-38ed-4bf5-b828-bb41a0d415b2","type":"DaysTicker"},{"id":"b43b9ed4-a1e9-462a-881c-5e318fb9aafe","type":"DaysTicker"},{"id":"3bf4c061-5d2f-4067-96aa-19138b8dc360","type":"MonthsTicker"},{"id":"577baa72-2584-421e-96a7-145834b7e44d","type":"MonthsTicker"},{"id":"2b87cd25-0a8a-4130-83c2-b5651ac2a2b9","type":"MonthsTicker"},{"id":"596fce41-3f17-4348-b4a8-0b94347a57ed","type":"MonthsTicker"},{"id":"a121e806-60c4-4e77-bf0c-84787a2ee993","type":"YearsTicker"}]},"id":"1b5096d7-2dd3-4ab5-9ec7-d0efbc28f463","type":"DatetimeTicker"},{"attributes":{"days":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},"id":"f6fdca82-13da-4926-a805-2aab51646c80","type":"DaysTicker"}],"root_ids":["ca854178-19e2-440a-a4f5-e532f51c8d20"]},"title":"Bokeh Application","version":"0.12.14"}}
                </script>
                <script type="text/javascript">
                    (function() {
                        var fn = function() {
                            Bokeh.safely(function() {
                                (function(root) {
                                    function embed_document(root) {

                                        var docs_json = document.getElementById('d71ebe9c-b9d7-4a09-a4e1-91cbd7717655').textContent;
                                        var render_items = [{"docid":"edb6f766-2d2f-49d6-8afa-dc46cffb690d","elementid":"035e5c7e-7aba-48a2-a387-b0170e954bab","modelid":"ca854178-19e2-440a-a4f5-e532f51c8d20"}];
                                        root.Bokeh.embed.embed_items(docs_json, render_items);

                                    }
                                    if (root.Bokeh !== undefined) {
                                        embed_document(root);
                                    } else {
                                        var attempts = 0;
                                        var timer = setInterval(function(root) {
                                            if (root.Bokeh !== undefined) {
                                                embed_document(root);
                                                clearInterval(timer);
                                            }
                                            attempts++;
                                            if (attempts > 100) {
                                                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing")
                                                clearInterval(timer);
                                            }
                                        }, 10, root)
                                    }
                                })(window);
                            });
                        };
                        if (document.readyState != "loading") fn();
                        else document.addEventListener("DOMContentLoaded", fn);
                    })();
                </script>
                <h2 class="section-heading">LSTM</h2>
                <p>As very clearly explained <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a>, LSTM are good option for time series prediction. They allow to put weights on different inputs, to decide which data point should be more preponderant in making an accurate prediction. </p>

                <!--D3js LSTM plot-->
                <p></p><figure class="l-body">
                <object id="yoursvg" data="img/drawing.svg" type="image/svg+xml" ></object>
                <script>
                    d3.xml("rect01.svg").mimeType("image/svg+xml").get(function(error, xml) {
                        if (error) throw error;
                        document.body.appendChild(xml.documentElement);
                    });

                    // d3.xml("img/attention.svg").mimeType("image/svg+xml").get(function(error, xml)
                    // {
                    //     if (error) throw error;
                    //     var importedNode = document.importNode(xml.documentElement, true);
                    //     d3.select("div#vis")
                    //         .each(function() {
                    //             this.appendChild(importedNode);
                    //         })
                    //     //document.body.appendChild(xml.documentElement);
                    //     //styleImportedSVG()
                    // };
                    //g.selectAll('rect')
                         //.filter(function(d) { return d.ArtistID == 1; })
                    //     .style('fill-opacity', 0.3)

                    // svg.append("g")
                    //     .on("mouseover", function() {
                    //         d3.select(this).classed("hover", true);
                    //     })
                    //     .on("mouseout", function() {
                    //         d3.select(this).classed("hover", false);
                    //     })
                    // svg.selectAll('rect[style = "fill-opacity: orange;"]')
                    //
                    // function styleImportedSVG () {
                    //     d3.select('svg')
                    //         .on('mouseover', function() {
                    //             console.log('mouseover');
                    //             console.log('this', this);
                    //             d3.selectAll('path')
                    //                 .style({
                    //                     'fill-opacity':   0.1,
                    //                     'stroke-opacity': 0.3
                    //                 })
                    //         })
                    //         .on('mouseout', function() {
                    //             console.log('mouseout');
                    //             d3.selectAll('path')
                    //                 .style({
                    //                     'fill-opacity':   1,
                    //                     'stroke-opacity': 1
                    //                 })
                    //         })
                    //}

                </script>
                <!--<dt-include src="assets/rnn_attentional_01.svg"><svg id="background" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 648 258">-->
                <!--<script>-->
                    <!--// Extract the width and height that was computed by CSS.-->
                    <!--var chartDiv = document.getElementById("vis");-->
                    <!--var width = chartDiv.clientWidth;-->
                    <!--var height = chartDiv.clientHeight;-->

                    <!--// load the external svg from a file-->
                    <!--d3.xml("img/attention.svg", "image/svg+xml", function(xml) {-->
                        <!--var importedNode = document.importNode(xml.documentElement, true);-->
                        <!--d3.select("div#vis")-->
                            <!--.each(function() {-->
                                <!--this.appendChild(importedNode);-->
                            <!--})-->
                        <!--// inside of our d3.xml callback, call another function-->
                        <!--// that styles individual paths inside of our imported svg-->
                        <!--styleImportedSVG()-->
                    <!--});-->

                    <!--function styleImportedSVG () {-->
                        <!--d3.select('svg')-->
                            <!--.on('mouseover', function() {-->
                                <!--console.log('mouseover');-->
                                <!--console.log('this', this);-->
                                <!--d3.selectAll('path')-->
                                    <!--.style({-->
                                        <!--'fill-opacity':   0.1,-->
                                        <!--'stroke-opacity': 0.3-->
                                    <!--})-->
                            <!--})-->
                            <!--.on('mouseout', function() {-->
                                <!--console.log('mouseout');-->
                                <!--d3.selectAll('path')-->
                                    <!--.style({-->
                                        <!--'fill-opacity':   1,-->
                                        <!--'stroke-opacity': 1-->
                                    <!--})-->
                            <!--})-->
                    <!--}-->
                <!--</script>-->

                <h2 class="section-heading">Attention Network</h2>
                <p></p>


                <h2 class="section-heading">Dilated convolution</h2>
                <p></p>
                <p>
                <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/primal-dual-net/loss_1.png" alt="">
                            <figcaption>Loss of this network with respect to the epoch number.</figcaption>
                        </figure>
                    </a>
                </p>
                <a href="#">
                    <figure>
                        <img class="img-responsive" src="img/primal-dual-net/results/2500_SboffQ_obs.png" alt="">
                        <figcaption>Lena Noised.</figcaption>
                    </figure>
                </a>

                <a href="#">
                    <figure>
                        <img class="img-responsive" src="img/primal-dual-net/results/2500_SboffQ_den.png" alt="">
                        <figcaption>Lena Denoised.</figcaption>
                    </figure>
                </a>

                <a href="#">
                    <figure>
                        <img class="img-responsive" src="img/image_Lena512.png" alt="">
                        <figcaption>Reference image for Lena.</figcaption>
                    </figure>
                </a>



                <p>It seems to denoise pretty nicely, even with a strong noise in the observed image. The average running time in the testing setup in \(0.06\) seconds. </p>

                <h2 class="section-heading">Python Code Example with PyTorch</h2>
                <p> Here is how I organized the code with PyTorch, with GPU support. Every function needed in the Primal Dual Net is coded as a nn.Module submodule:</p>

                <pre class="prettyprint">
    <code class="language-python">
from torch.autograd import Variable
import torch
import torch.nn as nn

class ForwardWeightedGradient(nn.Module):
    def __init__(self):
        super(ForwardWeightedGradient, self).__init__()

    def forward(self, x, w, dtype=torch.cuda.FloatTensor):
        """

        :param x: PyTorch Variable [1xMxN]
        :param w: PyTorch Variable [2xMxN]
        :param dtype: Tensor type
        :return: PyTorch Variable [2xMxN]
        """
        im_size = x.size()
        gradient = Variable(torch.zeros((2, im_size[1], im_size[2])).type(dtype))  # Allocate gradient array
        # Horizontal direction
        gradient[0, :, :-1] = x[0, :, 1:] - x[0, :, :-1]
        # Vertical direction
        gradient[1, :-1, :] = x[0, 1:, :] - x[0, :-1, :]
        gradient = gradient * w
        return gradient


class BackwardWeightedDivergence(nn.Module):
    def __init__(self):
        super(BackwardWeightedDivergence, self).__init__()

    def forward(self, y, w, dtype=torch.cuda.FloatTensor):
        """

        :param y: PyTorch Variable, [2xMxN], dual variable
        :param dtype: tensor type
        :return: PyTorch Variable, [1xMxN], divergence
        """
        im_size = y.size()
        y_w = w.cuda() * y
        # Horizontal direction
        d_h = Variable(torch.zeros((1, im_size[1], im_size[2])).type(dtype))
        d_h[0, :, 0] = y_w[0, :, 0]
        d_h[0, :, 1:-1] = y_w[0, :, 1:-1] - y_w[0, :, :-2]
        d_h[0, :, -1] = -y_w[0, :, -2:-1]

        # Vertical direction
        d_v = Variable(torch.zeros((1, im_size[1], im_size[2])).type(dtype))
        d_v[0, 0, :] = y_w[1, 0, :]
        d_v[0, 1:-1, :] = y_w[1, 1:-1, :] - y_w[1, :-2, :]
        d_v[0, -1, :] = -y_w[1, -2:-1, :]

        # Divergence
        div = d_h + d_v
        return div


class PrimalWeightedUpdate(nn.Module):
    def __init__(self, lambda_rof, tau):
        super(PrimalWeightedUpdate, self).__init__()
        self.backward_div = BackwardWeightedDivergence()
        self.tau = tau
        self.lambda_rof = lambda_rof

    def forward(self, x, y, img_obs, w):
        """

        :param x: PyTorch Variable [1xMxN]
        :param y: PyTorch Variable [2xMxN]
        :param img_obs: PyTorch Variable [1xMxN]
        :return:Pytorch Variable, [1xMxN]
        """
        x = (x + self.tau * self.backward_div.forward(y, w) +
             self.lambda_rof * self.tau * img_obs) / (1.0 + self.lambda_rof * self.tau)
        return x

class DualWeightedUpdate(nn.Module):
    def __init__(self, sigma):
        super(DualWeightedUpdate, self).__init__()
        self.forward_grad = ForwardWeightedGradient()
        self.sigma = sigma

    def forward(self, x_tilde, y, w):
        """

        :param x_tilde: PyTorch Variable, [1xMxN]
        :param y: PyTorch Variable, [2xMxN]
        :param w: PyTorch Variable, [2xMxN]
        :return: PyTorch Variable, [2xMxN]
        """
        y = y + self.sigma * self.forward_grad.forward(x_tilde, w)
        return y


class PrimalRegularization(nn.Module):
    def __init__(self, theta):
        super(PrimalRegularization, self).__init__()
        self.theta = theta

    def forward(self, x, x_tilde, x_old):
        """

        :param x: PyTorch Variable, [1xMxN]
        :param x_tilde: PyTorch Variable, [1xMxN]
        :param x_old: PyTorch Variable, [1xMxN]
        :return: PyTorch Variable, [1xMxN]
        """
        x_tilde = x + self.theta * (x - x_old)
        return x_tilde

class LinearOperator(nn.Module):
        """
        Neural Layers to learn the linear operator L.
        """
    def __init__(self):
        super(LinearOperator, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=1).cuda()
        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1).cuda()
        self.conv3 = nn.Conv2d(10, 2, kernel_size=3, stride=1, padding=1).cuda()

    def forward(self, x):
        """

        :param x:
        :return:
        """
        z = Variable(x.data.unsqueeze(0)).cuda()
        z = F.relu(self.conv1(z))
        z = F.relu(self.conv2(z))
        z = F.relu(self.conv3(z))
        y = Variable(z.data.squeeze(0).cuda())
        return y


class GaussianNoiseGenerator(nn.Module):
        """
        Gaussian noise generator for an image.
        """
    def __init__(self):
        super(GaussianNoiseGenerator, self).__init__()

    def forward(self, img, std, mean=0.0, dtype=torch.cuda.FloatTensor):
        """

        :param img:
        :param std:
        :param mean:
        :param dtype:
        :return:
        """
        noise = torch.zeros(img.size()).type(dtype)
        noise.normal_(mean, std=std)
        img_n = img + noise
        return img_n


class Net(nn.Module):

    def __init__(self, w1, w2, w, max_it, lambda_rof, sigma, tau, theta, dtype=torch.cuda.FloatTensor):
        super(Net, self).__init__()
        self.linear_op = LinearOperator()
        self.max_it = max_it
        self.dual_update = DualWeightedUpdate(sigma)
        self.prox_l_inf = ProximalLinfBall()
        self.primal_update = PrimalWeightedUpdate(lambda_rof, tau)
        self.primal_reg = PrimalRegularization(theta)

        self.pe = 0.0
        self.de = 0.0
        self.w1 = nn.Parameter(w1)
        self.w2 = nn.Parameter(w2)
        self.w = w
        self.clambda = nn.Parameter(lambda_rof.data)
        self.sigma = nn.Parameter(sigma.data)
        self.tau = nn.Parameter(tau.data)
        self.theta = nn.Parameter(theta.data)

        self.type = dtype

    def forward(self, x, img_obs):
        """

        :param x:
        :param img_obs:
        :return:
        """
        x = Variable(img_obs.data.clone()).cuda()
        x_tilde = Variable(img_obs.data.clone()).cuda()
        img_size = img_obs.size()
        y = Variable(torch.ones((img_size[0] + 1, img_size[1], img_size[2]))).cuda()

        # Forward pass
        y = self.linear_op(x)
        w_term = Variable(torch.exp(-torch.abs(y.data.expand_as(y))))
        self.w = self.w1.expand_as(y) + self.w2.expand_as(y) * w_term
        self.w.type(self.type)
        self.theta.data.clamp_(0, 5)
        for it in range(self.max_it):
            # Dual update
            y = self.dual_update.forward(x_tilde, y, self.w)
            y.data.clamp_(0, 1)
            y = self.prox_l_inf.forward(y, 1.0)
            # Primal update
            x_old = x
            x = self.primal_update.forward(x, y, img_obs, self.w)
            x.data.clamp_(0, 1)
            # Smoothing
            x_tilde = self.primal_reg.forward(x, x_tilde, x_old)
            x_tilde.data.clamp_(0, 1)

        return x_tilde
    </code>
</pre>
                <p>And now here is what the training script looks like:</p>
                <pre class="prettyprint">
    <code class="language-python">
import argparse
import random
import string
import os
import glob
import numpy as np
import matplotlib.pyplot as plt
import png
from PIL import Image


import torch
from torch.autograd import Variable

from torch.utils.data import DataLoader
from torchvision import transforms
import torch.nn as nn


from data_io import NonNoisyImages
from linear_operators import ForwardGradient, ForwardWeightedGradient
from primal_dual_models import Net, GaussianNoiseGenerator


def id_generator(size=6, chars=string.ascii_letters + string.digits):
    return ''.join(random.choice(chars) for _ in range(size))


def compute_mean_std_data(filelist):
    """

    :param filelist:
    :return:
    """
    tensor_list = []
    for file in filelist:
        img = Image.open(file)
        img_np = np.array(img).ravel()
        tensor_list.append(img_np.ravel())
    pixels = np.concatenate(tensor_list, axis=0)
    return np.mean(pixels), np.std(pixels)

parser = argparse.ArgumentParser(description='Run Primal Dual Net.')
parser.add_argument('--use_cuda', type=bool, default=True,
                        help='Flag to use CUDA, if available')
parser.add_argument('--max_it', type=int, default=20,
                        help='Number of iterations in the Primal Dual algorithm')
parser.add_argument('--max_epochs', type=int, default=5000,
                    help='Number of epochs in the Primal Dual Net')
parser.add_argument('--lambda_rof', type=float, default=5.,
                    help='Step parameter in the ROF model')
parser.add_argument('--theta', type=int, default=0.9,
                    help='Regularization parameter in the Primal Dual algorithm')
parser.add_argument('--tau', type=int, default=0.01,
                    help='Step Parameter in Primal')
parser.add_argument('--save_flag', type=bool, default=True,
                    help='Flag to save or not the result images')
parser.add_argument('--log', type=bool, help="Flag to log loss in tensorboard", default=False)
parser.add_argument('--out_folder', help="output folder for images",
                    default="guillaume_norm_20it_5k_epochs_15narrow_sigma_smooth_loss_lr_10-4/")
parser.add_argument('--clip', type=float, default=0.1,
                    help='Value of clip for gradient clipping')
args = parser.parse_args()

# Supplemental imports
if args.log:
    from tensorboard import SummaryWriter
    # Keep track of loss in tensorboard
    writer = SummaryWriter()
# Set parameters:
max_epochs = args.max_epochs
max_it = args.max_it
lambda_rof = args.lambda_rof
theta = args.theta
tau = args.tau
#sigma = 1. / (lambda_rof * tau)
sigma = 15.0
batch_size = 8
m, std =122.11/255., 53.55/255.
print(m, std)

# Transform dataset
transformations = transforms.Compose([transforms.Scale((512, 512)), transforms.ToTensor()])
dd = NonNoisyImages("/home/louise/src/blog/pytorch_primal_dual/images/BM3D/", transform=transformations)
#m, std = compute_mean_std_dataset(dd.data)
dtype = torch.cuda.FloatTensor

train_loader = DataLoader(dd,
                          batch_size=batch_size,
                          num_workers=4)
m1, n1 = compute_mean_std_data(train_loader.dataset.filelist)
print("m = ", m)
print("s = ", std)
# set up primal and dual variables
img_obs = Variable(train_loader.dataset[0])  # Init img_obs with first image in the data set
x = Variable(img_obs.data.clone().type(dtype))
x_tilde = Variable(img_obs.data.clone().type(dtype))
img_size = img_obs.size()
y = Variable(torch.zeros((img_size[0] + 1, img_size[1], img_size[2])).type(dtype))
y = ForwardGradient().forward(x)
g_ref = y.clone()

# Net approach
w1 = 0.5 * torch.ones([1]).type(dtype)
w2 = 0.4 * torch.ones([1]).type(dtype)
w = Variable(torch.rand(y.size()).type(dtype))
# Primal dual parameters as net parameters
lambda_rof = nn.Parameter(lambda_rof * torch.ones([1]).type(dtype))
sigma = nn.Parameter(sigma * torch.ones([1]).type(dtype))
tau = nn.Parameter(tau * torch.ones([1]).type(dtype))
theta = nn.Parameter(theta*torch.ones([1]).type(dtype))


n_w = torch.norm(w, 2, dim=0)
plt.figure()
plt.imshow(n_w.data.cpu().numpy())
plt.colorbar()
plt.title("Norm of Initial Weights of Gradient of Noised image")

net = Net(w1, w2, w, max_it, lambda_rof, sigma, tau, theta)

criterion = torch.nn.MSELoss(size_average=True)
criterion_g = torch.nn.MSELoss(size_average=True)
optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)
params = list(net.parameters())
loss_history = []
primal_history = []
dual_history = []
gap_history = []
it = 0
print(dd.filelist[0])
img_ref = Variable(train_loader.dataset[0]).type(dtype)
#std = 0.3 * torch.ones([1])
for t in range(max_epochs):
    # Pick random image in dataset

    img_ref = Variable(random.choice(train_loader.dataset)).type(dtype)
    #print(img_ref)
    y = ForwardGradient().forward(img_ref)
    # Pick random noise variance in the given range
    std = np.random.uniform(0.05, 0.1, 1)
    # Apply noise on chosen image
    img_obs = torch.clamp(GaussianNoiseGenerator().forward(img_ref.data, std[0]), min=0.0, max=1.0)
    img_obs = Variable(img_obs).type(dtype)
    x = Variable(img_obs.data.clone())
    w = Variable(torch.rand(y.size()).type(dtype))
    y = ForwardWeightedGradient().forward(x, w)

    # Forward pass: Compute predicted image by passing x to the model
    x_pred = net(x, img_obs)
    # Compute and print loss
    g_ref = Variable(ForwardWeightedGradient().forward(img_ref, net.w).data, requires_grad=False)
    loss_1 = 255. * criterion(x_pred, img_ref)
    loss_2 = 255. * criterion_g(ForwardWeightedGradient().forward(x_pred, net.w), g_ref)

    loss = loss_1 + loss_2
    loss_history.append(loss.data[0])
    print(t, loss.data[0])

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm(net.parameters(), args.clip)
    optimizer.step()
    if it % 5 == 0 and args.log:
        writer.add_scalar('loss', loss.data[0], it)
    it += 1

    if args.save_flag:
        base_name = id_generator()
        folder_name = args.out_folder
        if not os.path.isdir(folder_name):
            os.mkdir(folder_name)
        fn = folder_name + str(it) + "_" + base_name + "_obs.png"
        f = open(fn, 'wb')
        w1 = png.Writer(img_obs.size()[2], img_obs.size()[1], greyscale=True)
        w1.write(f, np.array(transforms.ToPILImage()(img_obs.data.cpu())))
        f.close()
        fn = folder_name + str(it) + "_" + base_name + "_den.png"
        f_res = open(fn, 'wb')
        w1.write(f_res, np.array(transforms.ToPILImage()(x_pred.data.cpu())))
        f_res.close()

print("w1 = ", net.w1.data[0])
print("w2 = ", net.w2.data[0])
print("tau = ", net.tau.data[0])
print("theta = ", net.theta.data[0])
print("sigma = ", net.sigma.data[0])

std = 0.1
# Apply noise on chosen image
img_obs = Variable(torch.clamp(GaussianNoiseGenerator().forward(img_ref.data, std), min=0., max=1.)).type(dtype)
lin_ref = ForwardWeightedGradient().forward(img_ref.type(dtype), net.w)
grd_ref = ForwardGradient().forward(img_ref)
img_den = net.forward(img_obs, img_obs).type(dtype)
lin_den = ForwardWeightedGradient()(img_den, net.w)
plt.figure()
n1 = torch.norm(lin_ref, 2, dim=0)
plt.imshow(n1.data.cpu().numpy())
plt.title("Linear operator applied on reference image")
plt.figure()
n2 = torch.norm(grd_ref, 2, dim=0)
plt.imshow(n2.data.cpu().numpy())
plt.title("Gradient operator applied on reference image")

n_w = torch.norm(net.w, 2, dim=0)
plt.figure()
plt.imshow(n_w.data.cpu().numpy())
plt.colorbar()
plt.title("Norm of Weights of Gradient of Noised image")

plt.figure()
plt.imshow(np.array(transforms.ToPILImage()((img_obs.data).cpu())))
plt.colorbar()
plt.title("noised image")

plt.figure()
plt.imshow(np.array(transforms.ToPILImage()((x_pred.data).cpu())))
plt.colorbar()
plt.title("denoised image")

    </code>
</pre>


                <p>The code (and images as well) is available in its full <a href="https://github.com/louisenaud/pytorch_primal_dual">here</a>. </p>

                <p>In the next part of this ticket, I will implement the general form of the aforementionned problem.</p>


                <p>You can leave comments down here, or contact me through the contact form of this blog if you
                    have questions or remarks on this post!</p>
                <!-- Blog Comments -->

                <!-- begin wwww.htmlcommentbox.com -->
                <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">Widget</a> is loading comments...</div>
                <link rel="stylesheet" type="text/css" href="//www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
                <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="//www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%24rbyy1KYA6pUs1Jh9BfMHZ."+"&opts=16862&num=10&ts=1498161055918");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
                <!-- end www.htmlcommentbox.com -->


            </div>
        </div>
    </div>
    </div>
    </div>
</article>

<hr>

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <li>
                        <a href="https://twitter.com/naud_louise">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/louise.naud1">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/louisenaud">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                </ul>
                <p class="copyright text-muted">Louise Naud 2017</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="vendor/jquery/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="vendor/bootstrap/js/bootstrap.min.js"></script>

<!-- Contact Form JavaScript -->
<script src="js/jqBootstrapValidation.js"></script>
<script src="js/contact_me.js"></script>

<!-- Theme JavaScript -->
<script src="js/clean-blog.min.js"></script>

<script>
    var blocks = document.getElementsByClassName("pseudocode");
    for(var blockId = 0; blockId < blocks.length; blockId++) {
        var block = blocks[blockId];

        var code = block.textContent;
        var options = {
            lineNumber: true
        };

        var outputEl = document.createElement('div');
        outputEl.className += " pseudocode-out";
        block.parentNode.insertBefore(outputEl, block.nextSibling);

        pseudocode.render(code, outputEl, options);
    }

    while( blocks[0]) {
        blocks[0].parentNode.removeChild(blocks[0]);
    }
</script>
<!-- Google Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52868500-2', 'auto');
    ga('send', 'pageview');

</script>

</body>

</html>
</title>
</head>
<body>

</body>
</html>
