<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Louise Naud">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- For code display: Code Prettify -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>


    <title>M3N and Deep Learning</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
    <!-- Configuration for Mathjax and AlgoType-->
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    extensions: ["tex2jax.js", "color.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] },
    TeX: {
        Macros: {
                    And:     "\\\\mathbf{and}",
                    Or:      "\\\\mathbf{or}",
                    Not:     "\\\\mathbf{not}",
                    Is:      "\\\\mathbf{is}",
                    In:      "\\\\mathbf{in}",
                    Mapped:  "\\\\mathbf{mapped}",
                    Nil:     "\\\\mathbf{nil}"
        }
    }
});
</script>
    <!-- KateX for math formulas in pseudo code -->
    <link rel="stylesheet" href="katex/katex.min.css">
    <script src="katex/katex.min.js"></script>
    <!-- Stylesheet for pseudo code -->
    <link rel="stylesheet" href="pseudocode/pseudocode.min.css">
    <script src="pseudocode/pseudocode.min.js"></script>
    <!-- Library for Pseudo Code -->
    <link rel="stylesheet" href="Algotype.js/algotype_my_config.css">
    <script type="text/javascript" async
            src="Algotype.js/algotype.js">
    </script>
</head>

<body>

<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                Menu <i class="fa fa-bars"></i>
            </button>
            <a class="navbar-brand" href="index.html">ML Food for the Soul</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="index.html">Home</a>
                </li>
                <li>
                    <a href="about.html">About</a>
                </li>
                <li>
                    <a href="blog-tickets-list.html">Posts</a>
                </li>
                <li>
                    <a href="contact.html">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

<!-- Page Header -->
<!-- Set your background image for this header on the line below. -->
<header class="intro-header" style="background-image: url('img/m3n/m3n_cg_header.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Max Margin Markov Network and Deep Learning</h1>
                    <h2 class="subheading">How to take good advantage of the structure in the image.</h2>
                    <span class="meta">Posted by <a href="contact.html">Louise</a> on January 7, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>In this post I am going to reformulate the work of my Master Thesis <a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxsb3Vpc2VuYXVkfGd4OjM2MTBkNDRjMzYxOWM0NDQ">Max Margin Markov Networks and Deep Neural Networks for Stereo Imaging</a>.
                </p>
                <p>The idea of my master Thesis is to mingle deep neural network and graphical model. We know that deep neural networks are great at creating high level features and that Graphical Models are an excellent tool for enforcing structure. In this work, we want to have the best of both approaches. To this end we rely on the Max Margin Markov Network (M3N) framework.  </p>

                <p>In my Master thesis at Caltech, I proposed a first instantiation of this idea. But it turned out to be very slow and computationally expansive. In this blog post, I revisit the original approach and I propose a equivalent formulation that is much faster.</p>

                <h2 class="section-heading">Graphical Models</h2>
                Graphical Models have been extensively used in Computer Vision under the form of Markov Random Field, MRF, and Conditional Random Fields, CRF. In this blog, we abusively use the notation MRF for either an MRF or a CRF.
                <p>
                An MRF has a support graph composed of nodes \( \mathcal{V} \) and edges \( \mathcal{E} \). A set of random variables \(x = (x_i)_{i \in \mathcal{V}} \) that all take value in some discrete space \( \mathcal{L} \) represents the configuration of each node. 
                A unary term \( u_i: \mathcal{L} \rightarrow \mathbb{R}\) encodes the cost of each node  \( i \in  \mathcal{V} \). A pairwise term \( p_{i, j}: \mathcal{L} \times \mathcal{L} \rightarrow \mathbb{R}\) encodes the cost of each edge \( (i,j) \in  \mathcal{E} \).
                The cost of the a given configuration \(x\) is given by:
                    $$
                    MRF(x) = \sum_{i \in \mathcal{V}} u_i (x_i) + \sum_{(i, j) \in \mathcal{E}} p_{i,j}(x_i, x_j)
                    $$

                The configuration of minimum cost of an MRF is called the Maximum A Posteriori configuration:
                	$$
                	x_{opt} = \underset{x}{\text{argmin}} MRF(x)
                    $$

                Computing the MAP is generally an NP-hard problem, so we are only guaranteed to find approximate solutions.
                </p>

               	<h2 class="section-heading">Stereo Matching</h2>
				In the context of stereo matching, we want to register a reference image, \( I_{ref} \) to a target image \( I_{tar} \). Each pixel of the reference image has along the epipolar line an apparent motion that is called disparity. Hence, we associate a node \( i \) and a discrete random variable \( x_i \) to each pixel of the reference image. The discrete random variable encodes a potential disparity that takes value in \( \mathcal{L} \subset \mathbb{N} \).

				<p>
				The unary terms encodes the likeliness of a potential disparity based on a similarity prior. Hence, they measure how a patch \( P_{ref}(i) \) of the reference image centered around pixel \( i \) is similar to the patch \( P_{tar}(i, x_i) \) of the large image centered around pixel \( i \) translated by disparity \( x_i \). The notion of similarity is encoded by a scoring function \( s \) parametrized by \( \theta \):
                    $$
                    u_i (x_i; \theta) = s(P_{ref}(i), P_{tar}(i, x_i); \theta)
                    $$

                    This scoring function can as simple as the sum of absolute difference of the two patches or it can be modeled by a Deep Neural Network as in the Paper of <a href="https://github.com/szagoruyko/cvpr15deepcompare">Serguey Szagoruyko</a> .

                </p>

                <p>
                The pairwise terms enforce a structure prior called regularization based on piecewise smoothness. To this end they penalize the gradient of the disparity.
                	$$
                	p_{i,j}(x_i, x_j; \lambda, \alpha) = w_{i, j}(I_{ref}; \lambda) \min(|x_i - x_j|, \alpha)
                	$$
                The strength of this regularization is modulated by \(w_{i, j}\) so that contours of the disparity align with contours of the reference image:
                    $$
                    w_{i, j} (I; \lambda) = \lambda_{cst} + \lambda_{grad} \exp (- \lambda_{\sigma^{-1}} |I(i) - I(j)|)
                    $$
                Again, we could have use a more complex regularization function where \( w_{i, j} \) will have been modeled as Deep Neural Network.
                </p>

                We now have an MRF that has a set of parameters \(\tau = (\theta, \alpha, \lambda) \). Our goal is to tune automatically those parameters.


                <h2 class="section-heading">What is a Max Margin Markov Network?</h2>
                <p> Let's assume we are given k-MRFs with associated optimal configurations \(x_{gt}^k \) and common parameters \(\tau \). We want to find the parameters \(\tau \) that will make, ideally, each \(x_{gt}^k \) the Maximum A Posteriori, MAP, solution for \(MRF^k \). Unfortunately since the \( \text{argmin} \) is not differentiable we cannot directly learn the parameters \(\tau \) by minimizing for instance:
					$$
                    \begin{align}
                    & \underset{\tau}{\text{minimize}} & \sum_{k} \| x^k_{gt} - \underset{x^k}{\text{argmin}} ~\ MRF^k(x^k; \tau) \|
                    \end{align}
                    $$

                However, while \( \text{argmin} \) is not differentiable the operator \( \text{min} \) is differentiable. The Max Margin Markov Network exploits this fact and it attempts to make the energy of ground truth of \( (MRF^k)_k \) to coincide with the MAP energy. To this end, it optimizes:
                    $$
                    \begin{align}
                    & \underset{\tau}{\text{minimize}} & &  R(\tau) + \sum_{k} [  MRF^k (x_{gt}^k ; \tau) - \underset{x_k}{\min} ( {MRF}^k (x^k ; \tau) - \Delta(x_{gt}^k, x^k) )]
                    \end{align}
                    $$
                with:
                <ul>
                    <li>\(R \) a regularization function of the parameters, optional.</li>
                    <li>\(\Delta\) is a margin function that make all solutions different from \(x^k_{gt}\) to have an higher cost. </li>
                </ul>
                One can see that the margin function try to enforce the shape of the energy manifold of \( (MRF^k)_k \).
                </p> 

                <p> Assuming that we can write  \( \underset{x_k}{\min} ( {MRF}^k (x^k ; \tau) - \Delta(x_{gt}^k, x^k) )] \) as a differentiable computational graph, then we can use backpropagation techniques to optimize over \( \tau \). This is were we can derive two approaches.

                </p> 


                <h2 class="section-heading">Master thesis approach</h2>

                <p> In my Master thesis, I used the dual decomposition solver from Nikos Komodakis to compute a lower bound of \( \underset{x_k}{\min} ( {MRF}^k (x^k ; \tau) - \Delta(x_{gt}^k, x^k) )] \). I embedded this solver directly in the computational graph of the M3N. This resulted in a large computational graph.

                </p>

                <p>
                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/M3N_NN_1.png" alt="">
                            <figcaption>Simple computational graph of our optimization problem.</figcaption>
                        </figure>
                    </a>
                </p>

                <p>As can be seen in this figure, this naive workflow requires backpropagating the M3N errors through the entire Neural Net and solver. This is very expansive, and also unnecessary...</p>


                <h2 class="section-heading">A more efficient approach</h2>

                <p>I recently revisited the definition of the computational graph. One can see that in the M3N, there is no need to backpropagate through the variables \( x_{opt}^k \). Let's first introduce:
                $$
                    \overline{MRF}^k (x_{opt}^k ; \tau) = \underset{x^k}{\min} {MRF}^k (x^k ; \tau) - \Delta(x_{gt}^k, x^k)
                $$

                We can now simplify the M3N optimization problem as:
                    $$
                    \begin{align}
                        & \underset{\tau}{\text{minimize}} & &   R(\tau) + \sum_k MRF^k (x_{gt}^k ; \tau) - \overline{MRF}^k (x_{opt}^k ; \tau)
                    \end{align}
                    $$
                </p>

                

                <p>Here is what its associated the computational graph looks like for k=1:</p>
                <p>
                <a href="#">
                    <figure>
                        <img class="img-responsive" src="img/m3n/m3n_computational_graph.png" alt="">
                        <figcaption>Simple computational graph of our optimization problem.</figcaption>
                    </figure>
                </a>
                </p>

                <p>We can observe that \(x^k_{opt}\) is only an input of the computational graph. Hence, we can use any algorithm/implementation for computing:
                $$
                x_{opt}^k = \underset{x^k}{argmin} ~\overline{MRF}^k (x^k ; \tau)
                $$

				Hence, the idea is to alternate between two steps:
				<ul>
                    <li>Estimating \( (x^k_{opt})_k \) with a fast non differentiable algorithm (red arrow in previous figure). </li>
                    <li>Update the parameters \( \tau \) using the backpropagation (green arrows in previous figure).</li>
                </ul>

                I also proceed to change the solver for estimating the MAP. Instead of the lower bound provided by Dual Decomposition, I am using a faster solver (one to two orders of magnitude), Fast-PD, that provides an upper bound of \( \underset{x_k}{\min} ( {MRF}^k (x^k ; \tau) - \Delta(x_{gt}^k, x^k) )] \). However, since we work with an upper bound it's now possible to have:
                $$
                MRF^k (x_{gt}^k ; \tau) \leq \overline{MRF}^k (x_{opt}^k ; \tau)
                $$

                This means that \( x_{gt}^k \) has a lower cost but that the solver was not able to reach it (remember MAP is in general an NP-hard problem).

                Hence, we simply safeguard the M3N cost as follow:
					$$
                    \begin{align}
                        & \underset{\tau}{\text{minimize}} & &   R(\tau) + \sum_k \max(MRF^k (x_{gt}^k ; \tau) - \overline{MRF}^k (x_{opt}^k ; \tau), 0)
                    \end{align}
                    $$

                </p>

                <h2 class="section-heading">Implementation</h2>
                <p>Thanks to the wonderful library <a href="">Pytorch</a>, we can model easily this new M3N formulation as a differentiable computational graph. 
                In details we model as a Pytorch module the computation of the energies of  \(MRF^k (x_{gt}^k ; \tau) \) and \(\overline{MRF}^k (x_{opt}^k ; \tau) \) along with the regularization cost of \( R(\tau) \).</p>

                <p>The variable $x_{opt}$ is only an input of the M3N Pytorch model. One can use a non differentiable algorithm to estimate $x_{opt}$. I used <a href="http://imagine.enpc.fr/~conejob/">Bruno Conejo's</a> C++ PhD implementation wrapped for Python of Nikos Komodakis' <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.7698&rep=rep1&type=pdf">FastPD</a> algorithm.</p>

                <h2 class="section-heading">A few results</h2>
                <p>Here is the loss of the network for 20 epochs, on the whole KITTI stereo dataset:
                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/loss_stereo.png" alt="">
                            <figcaption>Loss of this network with respect to the epoch number.</figcaption>
                        </figure>
                    </a>

                    The optimization of the network parameters does converge nicely. For the whole Middlebury 2001 dataset, 10 epochs and 6 pairs of \(432 \times 381 \) images, the experiment takes about 6 minutes (as opposed to a few hours with the previous approach) to run (with CUDA, GTX 1080 Ti, 2 CPU workers).
                </p>

                <p>Here are some results on the Middlebury 2001 dataset:
                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/res_mid_2001_mask_shuffle_0_9.png" alt="">
                            <figcaption>Estimated disparity for "barn 1".</figcaption>
                        </figure>
                    </a>

                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/res_mid_2001_mask_shuffle_1_9.png" alt="">
                            <figcaption>Estimated disparity for "barn 2".</figcaption>
                        </figure>
                    </a>
                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/res_mid_2001_mask_shuffle_3_9.png" alt="">
                            <figcaption>Estimated disparity for "bull".</figcaption>
                        </figure>
                    </a>

                    <a href="#">
                        <figure>
                            <img class="img-responsive" src="img/m3n/res_mid_2001_mask_shuffle_4_9.png" alt="">
                            <figcaption>Estimated disparity for "poster".</figcaption>
                        </figure>
                    </a>
                </p>
                <p>The results are also quite satisfying, considering that our scorer is very simple. In a following blog post, I will explore how this approach performs with a Neural Network for the scorer instead of this similarity function.</p>
                <p>You can find the code in its whole <a href="https://github.com/louisenaud/m3n_fast_pd">here</a>. </p>

                <p>You can leave comments down here, or contact me through the contact form of this blog if you
                    have questions or remarks on this post!</p>
                <!-- Blog Comments -->

                <!-- begin wwww.htmlcommentbox.com -->
                <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">HTML Comment Box</a> is loading comments...</div>
                <link rel="stylesheet" type="text/css" href="//www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
                <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="//www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%24rbyy1KYA6pUs1Jh9BfMHZ."+"&opts=16862&num=10&ts=1515205452716");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
                <!-- end www.htmlcommentbox.com -->

            </div>
        </div>
    </div>
</article>

<hr>

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <li>
                        <a href="https://twitter.com/naud_louise">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/louise.naud1">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/louisenaud">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                        </a>
                    </li>
                </ul>
                <p class="copyright text-muted">Louise Naud 2017</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="vendor/jquery/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="vendor/bootstrap/js/bootstrap.min.js"></script>

<!-- Contact Form JavaScript -->
<script src="js/jqBootstrapValidation.js"></script>
<script src="js/contact_me.js"></script>

<!-- Theme JavaScript -->
<script src="js/clean-blog.min.js"></script>

<script>
    var blocks = document.getElementsByClassName("pseudocode");
    for(var blockId = 0; blockId < blocks.length; blockId++) {
        var block = blocks[blockId];

        var code = block.textContent;
        var options = {
            lineNumber: true
        };

        var outputEl = document.createElement('div');
        outputEl.className += " pseudocode-out";
        block.parentNode.insertBefore(outputEl, block.nextSibling);

        pseudocode.render(code, outputEl, options);
    }

    while( blocks[0]) {
        blocks[0].parentNode.removeChild(blocks[0]);
    }
</script>
<!-- Google Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-52868500-2', 'auto');
    ga('send', 'pageview');

</script>
</body>
</html>

