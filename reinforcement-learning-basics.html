<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Louise Naud">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- For code display: Code Prettify -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>


    <title>Reinforcement Learning</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script type="text/javascript" async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <!-- Configuration for Mathjax and AlgoType-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            extensions: ["tex2jax.js", "color.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            "HTML-CSS": { availableFonts: ["TeX"] },
            TeX: {
                Macros: {
                            And:     "\\\\mathbf{and}",
                            Or:      "\\\\mathbf{or}",
                            Not:     "\\\\mathbf{not}",
                            Is:      "\\\\mathbf{is}",
                            In:      "\\\\mathbf{in}",
                            Mapped:  "\\\\mathbf{mapped}",
                            Nil:     "\\\\mathbf{nil}"
                }
            }
        });
    </script>
    <!-- KateX for math formulas in pseudo code -->
    <link rel="stylesheet" href="katex/katex.css">
    <!-- Stylesheet for pseudo code -->
    <link rel="stylesheet" href="pseudocode/static/pseudocode.css">
    <link rel="stylesheet" href="Algotype.js/algotype_my_config.css">
    <script type="text/javascript" async
            src="Algotype.js/algotype.js">
    </script>
</head>




<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand" href="index.html">ML Food for the Soul</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="about.html">About</a>
                    </li>
                    <li>
                        <a href="blog-tickets-list.html">Posts</a>
                    </li>
                    <li>
                        <a href="contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/post-bg.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>Fundamental Algorithms in Reinforcement Learning - I</h1>
                        <h2 class="subheading">Or learning a sequence of actions - when you know your environment.</h2>
                        <span class="meta">Posted by <a href="#">Louise</a> on June 23, 2017</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Post Content -->
    <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <p>Reinforcement Learning is a very rich and active research area in Machine Learning; it is
                        defined in the very excellent book <a
                                href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981"> Reinforcement Learning: An Introduction </a> as "computational approach to learning from interaction".</p>
                    In this post, we are going to go through the basics algorithms of Reinforcement Learning when the
                    environment is known, namely:
                    <ul>
                        <li>Value Iteration</li>
                        <li>Policy Iteration</li>
                        <li>Dynamic Programming</li>
                    </ul>

                    <h2 class="section-heading">The Learning Scenario</h2>
                    <p>There are two entities in this scenario: the <i>agent</i> and the <i>environment</i>. Both
                    entities interact through <i>actions</i>, <i>states</i> and <i>rewards</i>. The agent performs an
                        action, which has an influence on the environment, and he recieves two types of information:
                        his current state in the environment and a reward corresponding to his action in the given
                        state he was in, and the defined goal. The ultimate objective of the agent is to optimize his sequence of action, the <i>policy</i>.</p>
                    <a href="#">
                        <img class="img-responsive" src="img/reinforcement-learning-1.png" alt="">
                    </a>

                    <h2 class="section-heading">Useful Definitions</h2>
                    <p>Let's suppose we have a set of <i>Decision Epochs</i>, which is most likely discrete in
                    our case; it will be noted \( {0, 1, ..., T} \).   \( T \), the <i>Horizon</i>, might be finite
                        of infinite.</p>
                    <h3 class="subheading">Markov Decision Process</h3>
                    <p>A Reinforcement Learning task that satisfies the Markov Property is called a <i>Markov
                        Decision Process</i> (MDP). It is defined by:</p>
                    <ul>
                        <li>A set of States \( \mathcal{S} \)</li>
                        <li>A set of Actions \( \mathcal{A} \)</li>
                        <li>A transition Probability \( P(s_{t+1} = s' \vert s_t = s, a_t = a) \) : the probability
                            that the agent will be in state \( s' \) at \( t+1 \) when in state \( s \) at time \( t \)
                            and
                            performing action \( a \)</li>
                        <li>\( R^{a}_{(s, s')} \) is the expected immediate reward received after transitioning from
                            state
                            \( s \) to state \( s' \), due to action \( a \)</li>
                        <li>\( \gamma \) the discount factor; it determines the value at time \( t \) of the future
                            rewards from \( t+1 \) to \( T \)</li>
                    </ul>
                    <h3 class="subheading">Policy</h3>
                    <p>A <i>policy</i> in this context is a map from states to probability distributions over
                    actions
                    \( \pi : S \rightarrow P(A=a \vert \mathcal{S}) \).</p>

                    <h3 class="subheading">Return</h3>
                    <ul>
                        <li>The <i>Return</i> is defined as: \( R_t = \sum _{k=0}^{N-1}r_{t+k+1}\), </li>
                        <li>The <i>Discounted Return</i> is defined as: \( R_t = \sum _{k=0}^{T-1}\gamma^{k} r_{t+k+1}\)
                            ,</li>
                    </ul>

                    <h3 class="subheading">Value Function</h3>
                    <p>The <i>Value Function</i> gives an estimate of "how good" - in terms of expected return-
                    it is for the agent to be in a given state, or to perform a given action in a given state.</p>
                    <ul>
                        <li>The <i>Value Function</i> for a given policy \( \pi \) is defined as:  \( \forall s, V^{\pi
                            }(s) = \mathbb{E}[R_t \vert s_t=s] \). It is the expected return when starting in state
                            \(s\) and following \(pi\) thereafter.</li>
                        <li>We can also define the <i>Action Value Function</i> for policy \( \pi \) as: \( Q^{\pi}(s,
                            a) =
                            \mathbb{E}_{\pi} [R_t \vert s_t = s, a_t = a]\)</li>
                        <li>The <i>Optimal Value Function </i> is defined as: \( \forall s \in
                            \mathcal{S}, V^{*}(s)=\max \limits _{\pi
                            }V^{\pi }(s) \),</li>
                        <li>The <i>Optimal Action-Value Function</i> is defined as: \( \forall (s,a) \in
                            \mathcal{S} \times \mathcal{A}, Q^{*}(s, a )=\max \limits _{\pi
                            }Q^{\pi }(s, a) \).</li>
                    </ul>

                    <p>Now is probably a good time to mention this property, which will be essential in our
                        peregrinations: $$ \begin{align*} \forall s \in \mathcal{S}, V^{*}(s) = \max \limits_{a \in
                        \mathcal{A}}
                        \sum_{s' \in
                        \mathcal{S}} P_{(s,s')}^a [R_{(s,s')}^a + \gamma V^{*}(s')]
                        \end{align*} $$
                        This equation is called the Bellman's optimality equation. This can be proven using the
                        recursive relation in the definition of the Value Function and Return.
                    </p>
                    <p>A policy is optimal if it has maximal value \( \forall s \in \mathcal{S}\). We can write this
                    as: $$ \begin{align*} \forall s \in \mathcal{S}, \pi^{*}(s) = \arg \max \limits_{a \in \mathcal{A}}
                        Q^{*} (s,a) \end{align*}$$</p>


                    <h2 class="section-heading">Policy Iteration</h2>
                    <p>The goal of a reinforcement problem is to find the best course of actions. "Best" means in terms
                    of return, and the course of action is the policy in our context. Computing the value function
                    helps us finding better policies. In this blog post, we are interested in problems where we know
                    the environment, which means that the transition probabilities and expected rewards are know for
                    each pair of states, and each action. The most straightforward algorithm to come up with a policy
                        is the policy iteration.</p>

                    <p>The idea here is to alternate a policy evaluation with a policy improvement. This iteratively
                    refines the optimal policy. The proof of convergence is fairly straightforward through the
                        Bellman Operator, which we won't define in this post, but you can find the proof <a
                                href="http://chercheurs.lille.inria.fr/~ghavamza/RL-EC-Lille/Lecture4-b.pdf"> here
                        </a>.</p>


                    <p>Here is the pseudo code of the Policy Iteration:</p>

                    <p>
                        <alg-algorithm header="Policy-Iteration$(\pi_0, \mathcal{S}, \mathcal{A}, P, r)$"
                                comment="Policy Iteration Algorithm">
                            <alg-step comment="Arbitrary Initial Policy">$\pi \leftarrow \pi_0$</alg-step>
                            <alg-step>$\pi' \leftarrow \varnothing$</alg-step>
                            <alg-while condition="$\pi \neq \pi' $">
                                <alg-step comment="Policy Evaluation">$V \leftarrow V_{\pi}$</alg-step>
                                <alg-step comment="Update Current Policy">$ \pi' \leftarrow \pi$</alg-step>
                                <alg-step comment="Greedy Policy Improvement">$ \pi \leftarrow \arg \max_{\pi}
                                    R_{\pi} + \gamma P_{\pi} V $</alg-step>
                            </alg-while>
                            <alg-return comment="Return Estimated Optimal Policy">$ \pi$</alg-return>
                        </alg-algorithm>
                    </p>

                    <p>
                        <alg-algorithm header="Policy-Iteration$(\pi_0, \mathcal{S}, \mathcal{A}, P, r, \theta)$"
                                       comment="Policy Iteration Algorithm">
                            <alg-step comment="Arbitrary Initial Policy">$\pi \leftarrow \pi_0$</alg-step>
                            <alg-step>$\pi' \leftarrow \varnothing$</alg-step>
                            <alg-repeat-until condition="$\delta > \theta > 0$"
                                              comment="Policy Evaluation">
                                <alg-step>$\Delta \leftarrow 0$ </alg-step>
                                <alg-step>$V(s) \leftarrow \sum_{s' \in \mathcal{S}} P_{(s,s')}^a [R_{(s,s')}^a + \gamma V(s')]$</alg-step>
                                <alg-step>$\Delta \leftarrow \max (\Delta, \vert v - V(s) \vert) $</alg-step>
                            </alg-repeat-until>
                            <alg-return comment="Return Estimated Optimal Policy">$ \pi$</alg-return>
                        </alg-algorithm>
                    </p>
                    <h2 class="section-heading">Value Iteration</h2>
                    <p>The main issue with Policy Iteration is that it involves multiple sweepings of the state
                        space \( \mathcal{S} \), which may be problematic with very large state spaces. The idea here is
                        to truncate the Policy Evaluation after one sweep. This basically means using the Bellman's
                        optimality equation as an update rule: $$ \begin{align*} \forall s \in \mathcal{S},
                        V_{k+1}(s) = \max \limits_{a}
                        \sum_{s' \in
                        \mathcal{S}} P_{(s,s')}^a [R_{(s,s')}^a + \gamma V_{k}(s')]
                        \end{align*} $$
                        </p>
                    <p>This algorithms converges in more steps than the Policy Iteration, but has a lower complexity
                        .</p>


                    <h2 class="section-heading">Linear Programming</h2>

                    <p>For these of you who are familiar (and into) Optimization, the Bellman equation defines
                        an Linear Programming optimization problem (linear objective function and linear
                        constraints).</p>
                    $$
                    \begin{align*}
                    & \underset{V}{\text{minimize}}
                    & & \sum_{s\in \mathcal{S}} \alpha (s) V(s)\\
                    & \text{subject to}
                    & & \forall s \in \mathcal{S}, \forall a \in \mathcal{A}, V(s) \geq R_{(s,s')}^a + \gamma
                    \sum_{s' \in \mathcal{S}} P_{(s,s')}^a V(s')
                    \end{align*}

                    $$
                    <p> \( \alpha \) can be interpreted as a probability to be in state \( s \). The number of rows
                        (constraints) of
                        this problem is \( \vert S \vert \vert A \vert \), and the number of columns (variables) \(
                        \vert S \vert
                        \). The algorithms solving LPs usually have a complexity depending on the number of rows of
                        the problem, so we can use the dual formulation of this problem. </p>

                    $$
                    \begin{align*}
                    & \underset{\lambda}{\text{maximize}}
                    & & \sum_{s\in \mathcal{S}, a \in \mathcal{A}}  \lambda (s, a) R(s, a)\\
                    & \text{subject to}
                    & & \forall s' \in \mathcal{S},  \sum_{a \in \mathcal{A}}  \lambda (s', a)  = \alpha (s') + \gamma
                    \sum_{s \in \mathcal{S}, a \in \mathcal{A}} P_{(s,s')}^a \lambda (s, a) \\
                    & & & \forall s \in \mathcal{S}, \forall a \in \mathcal{A}, \lambda (s, a) \geq 0
                    \end{align*}
                    $$

                    <p>The first constrains ensures that we are maximizing the return, and the second constraint
                        makes sure that \( \lambda \) is a discounted sum of probabilities. The number of rows
                        (constraints) of this problem is now \( \vert S \vert \), and the number of columns
                        (variables) \( \vert S \vert \vert A \vert \), which is much more favorable here.</p>
                    <!-- Blog Comments -->

                    <!-- begin wwww.htmlcommentbox.com -->
                    <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">Widget</a> is loading comments...</div>
                    <link rel="stylesheet" type="text/css" href="//www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
                    <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="//www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%24rbyy1KYA6pUs1Jh9BfMHZ."+"&opts=16862&num=10&ts=1498161055918");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
                    <!-- end www.htmlcommentbox.com -->

                    <!-- Comments Form -->
                    <!--<div class="well">
                        <h4>Leave a Comment:</h4>
                        <form role="form">
                            <div class="form-group">
                                <textarea class="form-control" rows="3"></textarea>
                            </div>
                            <a href="louise.naud@gmail.com" class="button">Submit</a>
                            &lt;!&ndash; <button type="submit" class="btn btn-primary">Submit</button>&ndash;&gt;
                        </form>
                    </div>

                    <hr>
-->
                    <!-- Posted Comments -->

                    <!--&lt;!&ndash; Comment &ndash;&gt;
                    <div class="media">
                        <a class="pull-left" href="#">
                            <img class="media-object" src="http://placehold.it/64x64" alt="">
                        </a>
                        <div class="media-body">
                            <h4 class="media-heading">Start Bootstrap
                                <small>August 25, 2014 at 9:30 PM</small>
                            </h4>
                            Cras sit amet nibh libero, in gravida nulla. Nulla vel metus scelerisque ante sollicitudin commodo. Cras purus odio, vestibulum in vulputate at, tempus viverra turpis. Fusce condimentum nunc ac nisi vulputate fringilla. Donec lacinia congue felis in faucibus.
                        </div>
                    </div>

                    &lt;!&ndash; Comment &ndash;&gt;
                    <div class="media">
                        <a class="pull-left" href="#">
                            <img class="media-object" src="http://placehold.it/64x64" alt="">
                        </a>
                        <div class="media-body">
                            <h4 class="media-heading">Start Bootstrap
                                <small>August 25, 2014 at 9:30 PM</small>
                            </h4>
                            Cras sit amet nibh libero, in gravida nulla. Nulla vel metus scelerisque ante sollicitudin commodo. Cras purus odio, vestibulum in vulputate at, tempus viverra turpis. Fusce condimentum nunc ac nisi vulputate fringilla. Donec lacinia congue felis in faucibus.
                            &lt;!&ndash; Nested Comment &ndash;&gt;
                            <div class="media">
                                <a class="pull-left" href="#">
                                    <img class="media-object" src="http://placehold.it/64x64" alt="">
                                </a>
                                <div class="media-body">
                                    <h4 class="media-heading">Nested Start Bootstrap
                                        <small>August 25, 2014 at 9:30 PM</small>
                                    </h4>
                                    Cras sit amet nibh libero, in gravida nulla. Nulla vel metus scelerisque ante sollicitudin commodo. Cras purus odio, vestibulum in vulputate at, tempus viverra turpis. Fusce condimentum nunc ac nisi vulputate fringilla. Donec lacinia congue felis in faucibus.
                                </div>
                            </div>-->
                            <!-- End Nested Comment -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Your Website 2016</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
    <script src="/pseudocode/pseudocode.js"></script>
    <script>
        var blocks = document.getElementsByClassName("pseudocode");
        for(var blockId = 0; blockId < blocks.length; blockId++) {
            var block = blocks[blockId];

            var code = block.textContent;
            var options = {
                lineNumber: true
            };

            var outputEl = document.createElement('div');
            outputEl.className += " pseudocode-out";
            block.parentNode.insertBefore(outputEl, block.nextSibling);

            pseudocode.render(code, outputEl, options);
        }

        while( blocks[0]) {
            blocks[0].parentNode.removeChild(blocks[0]);
        }
    </script>

</body>

</html>
